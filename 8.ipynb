{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b7c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7402b05d-a334-43ca-a8d7-16fc1cfdd13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 11:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import split, col, count, size, format_string, input_file_name, element_at\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "dataset_path = \"BDAchallenge2324\" # 'hdfs://192.168.104.45:9000/user/amircoli/BDA2324'\n",
    "output_path = \"results\" # '/home/amircoli/BDAchallenge2324/results/5'\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa487252-3a2d-4708-8ab4-e1bd5b8b75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(year, station):\n",
    "    dataframe = spark.read.format('csv') \\\n",
    "        .option('header', 'true') \\\n",
    "        .load('{}/{}/{}'.format(dataset_path, year, station)) \\\n",
    "        .withColumn('year', element_at(split(input_file_name(), '/'), -2).cast('string')) \\\n",
    "        .withColumn('station', element_at(split(input_file_name(), '/'), -1).cast('string')) \\\n",
    "        .withColumn('station', split(col('station'), '.csv')[0])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8321407-a1c9-4594-8750-7ec4bff551f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(dataframe, file_name):\n",
    "    file_path = '{}/{}'.format(output_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        dataframe.coalesce(1).write.format('csv').option('header', 'true').save(file_path)\n",
    "    else:\n",
    "        dataframe.coalesce(1).write.format('csv').mode('overwrite').option('header', 'true').save(file_path)\n",
    "    print('File .csv esportato con successo in: {}/{}'.format(output_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5f66c6-c65d-4148-baeb-cb21eca2c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_txt(file_name, result):\n",
    "    with open('{}/{}'.format(output_path, file_name), \"w\") as file:\n",
    "        if isinstance(result, list):\n",
    "           file.write(\"\\n\".join(result))\n",
    "        else:\n",
    "           file.write(result)\n",
    "    print('File .txt esportato con successo in: {}/{}'.format(output_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8837f",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bef1a4",
   "metadata": {},
   "source": [
    "#### Stampare il numero di misurazioni effettuate per ogni anno per ogni stazione (ordinato per anno e stazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910c3853-f1ca-4971-8030-259df85034c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_task(dataframe):\n",
    "    rows = []\n",
    "    output_dataframe = dataframe.select('year', 'station') \\\n",
    "        .groupBy('year', 'station') \\\n",
    "        .agg(count('*').alias('measures_count')) \\\n",
    "        .orderBy('year', 'station') \n",
    "    for row in output_dataframe.collect():\n",
    "        row_values = '{}, {}, {}'.format(row['year'], row['station'], row['measures_count'])\n",
    "        rows.append(row_values)\n",
    "    export_txt(\"task1.txt\", rows)\n",
    "    #export_csv(output_dataframe, \"task1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c10982",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f5021",
   "metadata": {},
   "source": [
    "#### Stampare le prime 10 temperature (TMP) con il maggior numero di occorrenze ed il relativo conteggio registrate nell’area         evidenziata (ordinate per numero di occorrenze e temperatura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa94b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_task(dataframe):\n",
    "    rows = []\n",
    "    output_dataframe = dataframe \\\n",
    "        .filter((col('LATITUDE').between(30, 60)) & (col('LONGITUDE').between(-135, -90))) \\\n",
    "        .groupBy('TMP') \\\n",
    "        .agg(count('*').alias('TMP_count')) \\\n",
    "        .orderBy(col('TMP_count').desc(), col('TMP').desc()) \\\n",
    "        .limit(10)\n",
    "    for row in output_dataframe.collect():\n",
    "        row_values = '[(60,-135);(30,-90)], {}, {}'.format(float(row['TMP'][1:].replace(',', '.')), row['TMP_count'])\n",
    "        rows.append(row_values)\n",
    "    export_txt(\"task2.txt\", rows)\n",
    "    #export_csv(output_dataframe, \"task2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037c53f-d971-4508-89ac-cf252e76e697",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573f15d-ffc2-41dd-adc0-ca57c10f03fd",
   "metadata": {},
   "source": [
    "#### Stampare la stazione con la velocità in nodi che occorre più volte ed il relativo conteggio (ordinando per conteggio, velocità e stazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f16a6cf-9a14-41b0-b7e7-e61309ffff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_task(dataframe):\n",
    "    output_dataframe = dataframe \\\n",
    "        .withColumn('WND_speed', split(col('WND'), ',')[1]) \\\n",
    "        .groupBy('station', 'WND_speed') \\\n",
    "        .agg(count('*').alias('WND_speed_count')) \\\n",
    "        .orderBy(col('WND_speed_count').desc(), col('WND_speed').desc(), col('station').asc()) \\\n",
    "        .limit(1) \n",
    "    result = '{}, {}, {}'.format(output_dataframe.collect()[0]['station'], \\\n",
    "                                 output_dataframe.collect()[0]['WND_speed'], \\\n",
    "                                 output_dataframe.collect()[0]['WND_speed_count'])\n",
    "    export_txt(\"task3.txt\", result)\n",
    "    #export_csv(output_dataframe, \"task3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5abc2-129a-4823-9331-f7f2950b0456",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de24bc64-2f3d-4396-93de-9d5f59041233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File .txt esportato con successo in: results/task1.txt\n",
      "File .txt esportato con successo in: results/task2.txt\n",
      "File .txt esportato con successo in: results/task3.txt\n",
      "\n",
      "Tempo di completamento: 6.376797914505005 seconds.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    starting_time = time()\n",
    "    schema = StructType([])\n",
    "    total_dataframe = spark.createDataFrame([], schema)\n",
    "    for root, dirs, files in sorted(os.walk((dataset_path))):\n",
    "        for file in sorted(files):\n",
    "            if (file == '.DS_Store'):\n",
    "                continue\n",
    "            dataframe = read_csv(os.path.basename(root), file)\n",
    "            total_dataframe = total_dataframe.unionByName(dataframe, allowMissingColumns=True)\n",
    "    first_task(total_dataframe)  \n",
    "    second_task(total_dataframe)    \n",
    "    third_task(total_dataframe)\n",
    "    print('\\nTempo di completamento: {} seconds.'.format(time() - starting_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
